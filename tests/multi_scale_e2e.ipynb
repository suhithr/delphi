{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Scale Analysis End-to-End Test\n",
    "\n",
    "End-to-end test for multi-scale analysis integrated with the full Delphi pipeline.\n",
    "\n",
    "This runs a complete pipeline (cache -> construct -> explain -> score) and then\n",
    "performs multi-scale analysis on the cached activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-04 16:23:23 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "from delphi.__main__ import run\n",
    "from delphi.config import (\n",
    "    CacheConfig,\n",
    "    ConstructorConfig,\n",
    "    MultiScaleConfig,\n",
    "    RunConfig,\n",
    "    SamplerConfig,\n",
    ")\n",
    "from delphi.utils import base_path_cfg_aware\n",
    "from delphi.latents import LatentDataset\n",
    "from delphi.latents.latents import ActivationData\n",
    "from delphi.latents.loader import TensorBuffer\n",
    "from delphi.latents.multi_scale_analysis import compare_scales, summarize_multi_scale\n",
    "from delphi.latents.multi_scale_constructors import multi_scale_constructor\n",
    "from delphi.log.result_analysis import get_agg_metrics, load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure cache settings\n",
    "cache_cfg = CacheConfig(\n",
    "    dataset_repo=\"EleutherAI/fineweb-edu-dedup-10b\",\n",
    "    dataset_split=\"train[:5%]\",\n",
    "    dataset_column=\"text\",\n",
    "    batch_size=8,\n",
    "    cache_ctx_len=512,  # Large enough to test multiple scales\n",
    "    n_splits=5,\n",
    "    n_tokens=2_500_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure sampler\n",
    "sampler_cfg = SamplerConfig(\n",
    "    train_type=\"quantiles\",\n",
    "    test_type=\"quantiles\",\n",
    "    n_examples_train=40,\n",
    "    n_examples_test=50,\n",
    "    n_quantiles=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure constructor\n",
    "constructor_cfg = ConstructorConfig(\n",
    "    min_examples=90,\n",
    "    example_ctx_len=32,\n",
    "    n_non_activating=50,\n",
    "    non_activating_source=\"random\",\n",
    "    faiss_embedding_cache_enabled=True,\n",
    "    faiss_embedding_cache_dir=\".embedding_cache\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure multi-scale analysis\n",
    "multi_scale_cfg = MultiScaleConfig(\n",
    "    # context_sizes=[16, 32, 64, 128],  # Must all divide cache_ctx_len=256\n",
    "    n_examples_per_scale=50,\n",
    "    min_examples=10,\n",
    "    variance_threshold=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of hookpoints to test - sampling across different layers\n",
    "# pythia-160m has 12 layers (0-11)\n",
    "hookpoints_to_test = [\n",
    "    \"layers.0.mlp\",\n",
    "    \"layers.2.mlp\",\n",
    "    \"layers.4.mlp\",\n",
    "    \"layers.6.mlp\",\n",
    "    \"layers.8.mlp\",\n",
    "    \"layers.10.mlp\",\n",
    "    \"layers.11.mlp\",  # Final layer\n",
    "]\n",
    "\n",
    "# We'll store results for each hookpoint\n",
    "all_hookpoint_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Pipeline for Each Hookpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing hookpoint: layers.0.mlp\n",
      "============================================================\n",
      "\n",
      "Running full Delphi pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f347a3e1cad45d4bc514676d471ed82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 50 files:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving path for hookpoint: layers.0.mlp\n",
      "Overwriting results from /root/delphi/tests/results/test_multi_scale_layers_0_mlp/latents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2ce501e2d44bd9bb56e2c8756d0ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "167cb3532f6c40d796147209f1e91aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching latents: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 610/610 [00:10<00:00, 55.46it/s, Total Tokens=2,498,560]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping neighbour creation\n",
      "Overwriting results from /root/delphi/tests/results/test_multi_scale_layers_0_mlp/scores\n",
      "INFO 11-04 16:24:04 [utils.py:233] non-default args: {'max_model_len': 4208, 'enable_prefix_caching': True, 'disable_log_stats': True, 'model': 'hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4'}\n",
      "INFO 11-04 16:24:05 [model.py:547] Resolved architecture: LlamaForCausalLM\n",
      "INFO 11-04 16:24:05 [model.py:1510] Using max model len 4208\n",
      "INFO 11-04 16:24:05 [awq_marlin.py:119] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 11-04 16:24:05 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "WARNING 11-04 16:24:06 [__init__.py:3036] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
      "INFO 11-04 16:24:09 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=10608)\u001b[0;0m INFO 11-04 16:24:11 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=10608)\u001b[0;0m INFO 11-04 16:24:11 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4', speculative_config=None, tokenizer='hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4208, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=10608)\u001b[0;0m INFO 11-04 16:24:11 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=10608)\u001b[0;0m WARNING 11-04 16:24:12 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=10608)\u001b[0;0m INFO 11-04 16:24:12 [gpu_model_runner.py:2602] Starting to load model hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=10608)\u001b[0;0m INFO 11-04 16:24:12 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=10608)\u001b[0;0m INFO 11-04 16:24:12 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=10608)\u001b[0;0m INFO 11-04 16:24:13 [weight_utils.py:392] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  11% Completed | 1/9 [00:00<00:04,  1.88it/s]\n",
      "Loading safetensors checkpoint shards:  22% Completed | 2/9 [00:01<00:04,  1.64it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 3/9 [00:01<00:03,  1.58it/s]\n",
      "Loading safetensors checkpoint shards:  44% Completed | 4/9 [00:02<00:03,  1.55it/s]\n",
      "Loading safetensors checkpoint shards:  56% Completed | 5/9 [00:03<00:02,  1.53it/s]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 6/9 [00:03<00:01,  1.52it/s]\n",
      "Loading safetensors checkpoint shards:  78% Completed | 7/9 [00:04<00:01,  1.51it/s]\n",
      "Loading safetensors checkpoint shards:  89% Completed | 8/9 [00:05<00:00,  1.64it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:05<00:00,  1.95it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:05<00:00,  1.69it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=10608)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=10608)\u001b[0;0m INFO 11-04 16:24:19 [default_loader.py:267] Loading weights took 5.38 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=10608)\u001b[0;0m INFO 11-04 16:24:22 [gpu_model_runner.py:2653] Model loading took 37.0899 GiB and 9.263342 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=10608)\u001b[0;0m INFO 11-04 16:24:33 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/0bb9f9cc9a/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=10608)\u001b[0;0m INFO 11-04 16:24:33 [backends.py:559] Dynamo bytecode transform time: 11.09 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=10608)\u001b[0;0m INFO 11-04 16:24:38 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 3.325 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=10608)\u001b[0;0m INFO 11-04 16:24:45 [monitor.py:34] torch.compile takes 11.09 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=10608)\u001b[0;0m INFO 11-04 16:24:47 [gpu_worker.py:298] Available KV cache memory: 83.63 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=10608)\u001b[0;0m INFO 11-04 16:24:48 [kv_cache_utils.py:1087] GPU KV cache size: 274,032 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=10608)\u001b[0;0m INFO 11-04 16:24:48 [kv_cache_utils.py:1091] Maximum concurrency for 4,208 tokens per request: 65.12x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:10<00:00,  6.40it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:10<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=10608)\u001b[0;0m INFO 11-04 16:25:09 [gpu_model_runner.py:3480] Graph capturing finished in 22 secs, took 1.69 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=10608)\u001b[0;0m INFO 11-04 16:25:10 [core.py:210] init engine (profile, create kv cache, warmup model) took 47.89 seconds\n",
      "INFO 11-04 16:25:11 [llm.py:306] Supported_tasks: ['generate']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 0it [00:00, ?it/s]Not enough examples to explain the latent: 44\n",
      "Not enough examples to explain the latent: 16\n",
      "Processing items: 2it [01:47, 44.76s/it] Not enough examples to explain the latent: 64\n",
      "Processing items: 46it [05:18,  5.48s/it]Not enough examples to explain the latent: 32\n",
      "Processing items: 68it [07:06,  5.11s/it]Not enough examples to explain the latent: 88\n",
      "Processing items: 95it [07:46,  4.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline completed in 559.66 seconds\n",
      "\n",
      "============================================================\n",
      "Testing hookpoint: layers.2.mlp\n",
      "============================================================\n",
      "\n",
      "Running full Delphi pipeline...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6249b21f5a8445395070edab1eb1a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 50 files:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n",
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving path for hookpoint: layers.2.mlp\n",
      "Overwriting results from /root/delphi/tests/results/test_multi_scale_layers_2_mlp/latents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21b04fe45744504a24f01dafa60baec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b43145428cc4a6ca1b0ba5184ab0995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching latents: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 610/610 [00:10<00:00, 59.11it/s, Total Tokens=2,498,560]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping neighbour creation\n",
      "Overwriting results from /root/delphi/tests/results/test_multi_scale_layers_2_mlp/scores\n",
      "INFO 11-04 16:33:20 [utils.py:233] non-default args: {'max_model_len': 4208, 'enable_prefix_caching': True, 'disable_log_stats': True, 'model': 'hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4'}\n",
      "INFO 11-04 16:33:21 [model.py:547] Resolved architecture: LlamaForCausalLM\n",
      "INFO 11-04 16:33:21 [model.py:1510] Using max model len 4208\n",
      "INFO 11-04 16:33:21 [awq_marlin.py:119] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 11-04 16:33:21 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 11-04 16:33:25 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m INFO 11-04 16:33:27 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m INFO 11-04 16:33:27 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4', speculative_config=None, tokenizer='hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4208, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m INFO 11-04 16:33:27 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m ERROR 11-04 16:33:28 [core.py:708] EngineCore failed to start.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m ERROR 11-04 16:33:28 [core.py:708] Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m ERROR 11-04 16:33:28 [core.py:708]   File \"/root/delphi/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 699, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m ERROR 11-04 16:33:28 [core.py:708]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m ERROR 11-04 16:33:28 [core.py:708]   File \"/root/delphi/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 498, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m ERROR 11-04 16:33:28 [core.py:708]     super().__init__(vllm_config, executor_class, log_stats,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m ERROR 11-04 16:33:28 [core.py:708]   File \"/root/delphi/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 83, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m ERROR 11-04 16:33:28 [core.py:708]     self.model_executor = executor_class(vllm_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m ERROR 11-04 16:33:28 [core.py:708]   File \"/root/delphi/.venv/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 54, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m ERROR 11-04 16:33:28 [core.py:708]     self._init_executor()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m ERROR 11-04 16:33:28 [core.py:708]   File \"/root/delphi/.venv/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 54, in _init_executor\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m ERROR 11-04 16:33:28 [core.py:708]     self.collective_rpc(\"init_device\")\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m ERROR 11-04 16:33:28 [core.py:708]   File \"/root/delphi/.venv/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 83, in collective_rpc\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m ERROR 11-04 16:33:28 [core.py:708]     return [run_method(self.driver_worker, method, args, kwargs)]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m ERROR 11-04 16:33:28 [core.py:708]   File \"/root/delphi/.venv/lib/python3.10/site-packages/vllm/utils/__init__.py\", line 3122, in run_method\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m ERROR 11-04 16:33:28 [core.py:708]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m ERROR 11-04 16:33:28 [core.py:708]   File \"/root/delphi/.venv/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 259, in init_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m ERROR 11-04 16:33:28 [core.py:708]     self.worker.init_device()  # type: ignore\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m ERROR 11-04 16:33:28 [core.py:708]   File \"/root/delphi/.venv/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\", line 187, in init_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m ERROR 11-04 16:33:28 [core.py:708]     raise ValueError(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m ERROR 11-04 16:33:28 [core.py:708] ValueError: Free memory on device (5.51/139.8 GiB) on startup is less than desired GPU memory utilization (0.9, 125.82 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m Process EngineCore_DP0:\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m   File \"/root/.local/share/uv/python/cpython-3.10.19-linux-x86_64-gnu/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m     self.run()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m   File \"/root/.local/share/uv/python/cpython-3.10.19-linux-x86_64-gnu/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m   File \"/root/delphi/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 712, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m     raise e\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m   File \"/root/delphi/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 699, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m   File \"/root/delphi/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 498, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m     super().__init__(vllm_config, executor_class, log_stats,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m   File \"/root/delphi/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 83, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m   File \"/root/delphi/.venv/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 54, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m     self._init_executor()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m   File \"/root/delphi/.venv/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 54, in _init_executor\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m     self.collective_rpc(\"init_device\")\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m   File \"/root/delphi/.venv/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 83, in collective_rpc\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m     return [run_method(self.driver_worker, method, args, kwargs)]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m   File \"/root/delphi/.venv/lib/python3.10/site-packages/vllm/utils/__init__.py\", line 3122, in run_method\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m   File \"/root/delphi/.venv/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 259, in init_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m     self.worker.init_device()  # type: ignore\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m   File \"/root/delphi/.venv/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\", line 187, in init_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m     raise ValueError(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15529)\u001b[0;0m ValueError: Free memory on device (5.51/139.8 GiB) on startup is less than desired GPU memory utilization (0.9, 125.82 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.\n",
      "[rank0]:[W1104 16:33:28.753991641 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "# Loop through each hookpoint\n",
    "for hookpoint in hookpoints_to_test:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing hookpoint: {hookpoint}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Configure run for this hookpoint\n",
    "    run_cfg = RunConfig(\n",
    "        name=f\"test_multi_scale_{hookpoint.replace('.', '_')}\",\n",
    "        overwrite=[\"cache\", \"scores\"],\n",
    "        model=\"EleutherAI/pythia-160m\",\n",
    "        sparse_model=\"EleutherAI/sae-pythia-160m-32k\",\n",
    "        hookpoints=[hookpoint],\n",
    "        explainer_model=\"hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4\",\n",
    "        explainer_model_max_len=4208,\n",
    "        max_latents=100,\n",
    "        seed=22,\n",
    "        num_gpus=torch.cuda.device_count(),\n",
    "        filter_bos=True,\n",
    "        verbose=False,\n",
    "        sampler_cfg=sampler_cfg,\n",
    "        constructor_cfg=constructor_cfg,\n",
    "        cache_cfg=cache_cfg,\n",
    "        multi_scale_cfg=multi_scale_cfg,\n",
    "    )\n",
    "    \n",
    "    # Run the full pipeline\n",
    "    print(\"Running full Delphi pipeline...\")\n",
    "    start_time = time.time()\n",
    "    await run(run_cfg)\n",
    "    base_path = base_path_cfg_aware(run_cfg)\n",
    "    pipeline_time = time.time() - start_time\n",
    "    print(f\"Pipeline completed in {pipeline_time:.2f} seconds\")\n",
    "    \n",
    "    # Store the run config and base path for later analysis\n",
    "    all_hookpoint_results[hookpoint] = {\n",
    "        \"run_cfg\": run_cfg,\n",
    "        \"base_path\": base_path,\n",
    "        \"pipeline_time\": pipeline_time,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Scale Analysis for Each Hookpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now analyze multi-scale characteristics for each hookpoint\n",
    "n_latents_to_analyze = 50  # Analyze more latents for better statistics\n",
    "\n",
    "for hookpoint, result_data in all_hookpoint_results.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Multi-scale analysis for hookpoint: {hookpoint}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    run_cfg = result_data[\"run_cfg\"]\n",
    "    base_path = result_data[\"base_path\"]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    latents_path = base_path / \"latents\"\n",
    "    \n",
    "    # Use LatentDataset to properly load cached data\n",
    "    dataset = LatentDataset(\n",
    "        raw_dir=latents_path,\n",
    "        sampler_cfg=sampler_cfg,\n",
    "        constructor_cfg=constructor_cfg,\n",
    "    )\n",
    "    \n",
    "    # Load tokens\n",
    "    tokens = dataset.load_tokens()\n",
    "    print(f\"Loaded tokens: shape {tokens.shape}\")\n",
    "    \n",
    "    # Analyze latents from the dataset\n",
    "    multi_scale_results = []\n",
    "    \n",
    "    for i, buffer in enumerate(dataset.buffers):\n",
    "        if len(multi_scale_results) >= n_latents_to_analyze:\n",
    "            break\n",
    "        \n",
    "        # Load activations from this buffer\n",
    "        latents, split_locations, split_activations = buffer.load_data_per_latent()\n",
    "        \n",
    "        for latent_idx, locations, activations in zip(latents, split_locations, split_activations):\n",
    "            if len(multi_scale_results) >= n_latents_to_analyze:\n",
    "                break\n",
    "            \n",
    "            # Create ActivationData\n",
    "            activation_data = ActivationData(locations, activations)\n",
    "            \n",
    "            # Check if enough activations\n",
    "            if len(activation_data.activations) < multi_scale_cfg.min_examples:\n",
    "                print(f\"NOT ENOUGH ACTIVATION DATA in latent_idx {latent_idx}\")\n",
    "                continue\n",
    "            \n",
    "            # Run multi-scale constructor\n",
    "            multi_scale_data = multi_scale_constructor(\n",
    "                activation_data=activation_data,\n",
    "                tokens=tokens,\n",
    "                context_sizes=multi_scale_cfg.context_sizes,\n",
    "                cache_ctx_len=cache_cfg.cache_ctx_len,\n",
    "                n_examples_per_scale=multi_scale_cfg.n_examples_per_scale,\n",
    "                min_examples=multi_scale_cfg.min_examples,\n",
    "            )\n",
    "            \n",
    "            # Check if we got examples at multiple scales\n",
    "            non_empty_scales = [\n",
    "                ctx for ctx in multi_scale_cfg.context_sizes if multi_scale_data[ctx]\n",
    "            ]\n",
    "            \n",
    "            if len(non_empty_scales) >= 2:\n",
    "                # Run comparison\n",
    "                comparison = compare_scales(multi_scale_data)\n",
    "                summary = summarize_multi_scale(multi_scale_data)\n",
    "                \n",
    "                multi_scale_results.append(\n",
    "                    {\n",
    "                        \"latent_idx\": int(latent_idx),\n",
    "                        \"scale_type\": summary[\"scale_type\"],\n",
    "                        \"dominant_scale\": summary[\"dominant_scale\"],\n",
    "                        \"activation_variance\": summary[\"activation_variance\"],\n",
    "                        \"max_growth_ratio\": summary[\"max_growth_ratio\"],\n",
    "                        \"max_correlation\": summary[\"max_correlation\"],\n",
    "                    }\n",
    "                )\n",
    "    \n",
    "    multi_scale_time = time.time() - start_time\n",
    "    print(f\"Multi-scale analysis completed in {multi_scale_time:.2f} seconds\")\n",
    "    print(f\"Analyzed {len(multi_scale_results)} latents\")\n",
    "    \n",
    "    # Store results\n",
    "    result_data[\"multi_scale_results\"] = multi_scale_results\n",
    "    result_data[\"multi_scale_time\"] = multi_scale_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Hookpoints by Scale Type Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze which hookpoints are better at detecting longer-scale features\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Scale type hierarchy: token < phrase < sentence < paragraph\n",
    "scale_hierarchy = {\n",
    "    \"token\": 1,\n",
    "    \"phrase\": 2,\n",
    "    \"sentence\": 3,\n",
    "    \"paragraph\": 4,\n",
    "    \"unknown\": 0,\n",
    "}\n",
    "\n",
    "# Collect statistics for each hookpoint\n",
    "hookpoint_stats = []\n",
    "\n",
    "for hookpoint, result_data in all_hookpoint_results.items():\n",
    "    multi_scale_results = result_data[\"multi_scale_results\"]\n",
    "    \n",
    "    if not multi_scale_results:\n",
    "        print(f\"Warning: No multi-scale results for {hookpoint}\")\n",
    "        continue\n",
    "    \n",
    "    # Count scale types\n",
    "    scale_types = [r[\"scale_type\"] for r in multi_scale_results]\n",
    "    scale_counter = Counter(scale_types)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    dominant_scales = [r[\"dominant_scale\"] for r in multi_scale_results]\n",
    "    avg_dominant_scale = sum(dominant_scales) / len(dominant_scales)\n",
    "    \n",
    "    # Calculate proportion of longer features (sentence or paragraph)\n",
    "    long_features = sum(1 for st in scale_types if st in [\"sentence\", \"paragraph\"])\n",
    "    long_features_pct = 100 * long_features / len(scale_types)\n",
    "    \n",
    "    # Calculate average scale hierarchy score\n",
    "    avg_scale_score = sum(scale_hierarchy[st] for st in scale_types) / len(scale_types)\n",
    "    \n",
    "    # Calculate other metrics\n",
    "    avg_variance = sum(r[\"activation_variance\"] for r in multi_scale_results) / len(multi_scale_results)\n",
    "    avg_growth_ratio = sum(r[\"max_growth_ratio\"] for r in multi_scale_results) / len(multi_scale_results)\n",
    "    avg_correlation = sum(r[\"max_correlation\"] for r in multi_scale_results) / len(multi_scale_results)\n",
    "    \n",
    "    hookpoint_stats.append({\n",
    "        \"hookpoint\": hookpoint,\n",
    "        \"n_latents\": len(multi_scale_results),\n",
    "        \"avg_dominant_scale\": avg_dominant_scale,\n",
    "        \"long_features_pct\": long_features_pct,\n",
    "        \"avg_scale_score\": avg_scale_score,\n",
    "        \"token_count\": scale_counter.get(\"token\", 0),\n",
    "        \"phrase_count\": scale_counter.get(\"phrase\", 0),\n",
    "        \"sentence_count\": scale_counter.get(\"sentence\", 0),\n",
    "        \"paragraph_count\": scale_counter.get(\"paragraph\", 0),\n",
    "        \"unknown_count\": scale_counter.get(\"unknown\", 0),\n",
    "        \"avg_activation_variance\": avg_variance,\n",
    "        \"avg_growth_ratio\": avg_growth_ratio,\n",
    "        \"avg_correlation\": avg_correlation,\n",
    "    })\n",
    "\n",
    "# Create DataFrame for easier analysis\n",
    "stats_df = pd.DataFrame(hookpoint_stats)\n",
    "stats_df = stats_df.sort_values(\"long_features_pct\", ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HOOKPOINT COMPARISON: BETTER FOR LONGER FEATURES\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nRanked by percentage of sentence/paragraph features:\\n\")\n",
    "print(stats_df[[\"hookpoint\", \"n_latents\", \"long_features_pct\", \"avg_dominant_scale\", \"avg_scale_score\"]].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED SCALE TYPE DISTRIBUTION\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "print(stats_df[[\"hookpoint\", \"token_count\", \"phrase_count\", \"sentence_count\", \"paragraph_count\", \"unknown_count\"]].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ADDITIONAL METRICS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "print(stats_df[[\"hookpoint\", \"avg_activation_variance\", \"avg_growth_ratio\", \"avg_correlation\"]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results Across Hookpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Stacked bar chart of scale type distribution\n",
    "ax1 = axes[0, 0]\n",
    "hookpoints_ordered = stats_df[\"hookpoint\"].values\n",
    "x_pos = np.arange(len(hookpoints_ordered))\n",
    "\n",
    "token_counts = stats_df[\"token_count\"].values\n",
    "phrase_counts = stats_df[\"phrase_count\"].values\n",
    "sentence_counts = stats_df[\"sentence_count\"].values\n",
    "paragraph_counts = stats_df[\"paragraph_count\"].values\n",
    "\n",
    "ax1.bar(x_pos, token_counts, label='Token', color='#e74c3c')\n",
    "ax1.bar(x_pos, phrase_counts, bottom=token_counts, label='Phrase', color='#f39c12')\n",
    "ax1.bar(x_pos, phrase_counts + sentence_counts, bottom=token_counts + phrase_counts, \n",
    "        label='Sentence', color='#3498db')\n",
    "ax1.bar(x_pos, paragraph_counts, \n",
    "        bottom=token_counts + phrase_counts + sentence_counts,\n",
    "        label='Paragraph', color='#2ecc71')\n",
    "\n",
    "ax1.set_xlabel('Hookpoint', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Number of Latents', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Scale Type Distribution Across Hookpoints', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels([h.replace('layers.', 'L') for h in hookpoints_ordered], rotation=45)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Percentage of long features (sentence + paragraph)\n",
    "ax2 = axes[0, 1]\n",
    "colors = ['#2ecc71' if pct > 50 else '#e74c3c' for pct in stats_df[\"long_features_pct\"]]\n",
    "bars = ax2.barh(range(len(hookpoints_ordered)), stats_df[\"long_features_pct\"].values, color=colors)\n",
    "ax2.set_yticks(range(len(hookpoints_ordered)))\n",
    "ax2.set_yticklabels([h.replace('layers.', 'L') for h in hookpoints_ordered])\n",
    "ax2.set_xlabel('Long Features (%)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Percentage of Sentence/Paragraph Features\\n(Higher = Better for Long Features)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.axvline(x=50, color='gray', linestyle='--', linewidth=2, alpha=0.5)\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add percentage labels\n",
    "for i, (bar, pct) in enumerate(zip(bars, stats_df[\"long_features_pct\"].values)):\n",
    "    ax2.text(pct + 1, i, f'{pct:.1f}%', va='center', fontsize=10)\n",
    "\n",
    "# Plot 3: Average dominant scale\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(range(len(hookpoints_ordered)), stats_df[\"avg_dominant_scale\"].values, \n",
    "         marker='o', linewidth=2, markersize=8, color='#3498db')\n",
    "ax3.set_xticks(range(len(hookpoints_ordered)))\n",
    "ax3.set_xticklabels([h.replace('layers.', 'L') for h in hookpoints_ordered], rotation=45)\n",
    "ax3.set_xlabel('Hookpoint', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('Average Dominant Scale (tokens)', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Average Context Size of Dominant Scale', fontsize=14, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Average scale score (weighted by hierarchy)\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(range(len(hookpoints_ordered)), stats_df[\"avg_scale_score\"].values,\n",
    "         marker='s', linewidth=2, markersize=8, color='#9b59b6')\n",
    "ax4.set_xticks(range(len(hookpoints_ordered)))\n",
    "ax4.set_xticklabels([h.replace('layers.', 'L') for h in hookpoints_ordered], rotation=45)\n",
    "ax4.set_xlabel('Hookpoint', fontsize=12, fontweight='bold')\n",
    "ax4.set_ylabel('Average Scale Score', fontsize=12, fontweight='bold')\n",
    "ax4.set_title('Weighted Scale Score\\n(1=token, 2=phrase, 3=sentence, 4=paragraph)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('hookpoint_scale_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved as 'hookpoint_scale_analysis.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Best Hookpoints for Long Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY: BEST HOOKPOINTS FOR LONG FEATURES\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Top 3 hookpoints by long features percentage\n",
    "top_3 = stats_df.head(3)\n",
    "\n",
    "print(\"TOP 3 HOOKPOINTS (by % of sentence/paragraph features):\\n\")\n",
    "for idx, row in top_3.iterrows():\n",
    "    layer_num = row[\"hookpoint\"].split('.')[1]\n",
    "    print(f\"{row['hookpoint']}:\")\n",
    "    print(f\"  - Long features: {row['long_features_pct']:.1f}%\")\n",
    "    print(f\"  - Avg dominant scale: {row['avg_dominant_scale']:.1f} tokens\")\n",
    "    print(f\"  - Scale score: {row['avg_scale_score']:.2f}\")\n",
    "    print(f\"  - Distribution: {row['sentence_count']} sentence, {row['paragraph_count']} paragraph, \" +\n",
    "          f\"{row['phrase_count']} phrase, {row['token_count']} token\")\n",
    "    print()\n",
    "\n",
    "# Layer analysis\n",
    "early_layers = stats_df[stats_df[\"hookpoint\"].str.contains(\"layers\\.[0-3]\\.\")]\n",
    "middle_layers = stats_df[stats_df[\"hookpoint\"].str.contains(\"layers\\.[4-7]\\.\")]\n",
    "late_layers = stats_df[stats_df[\"hookpoint\"].str.contains(\"layers\\.[8-9]\\.\") | \n",
    "                        stats_df[\"hookpoint\"].str.contains(\"layers\\.1[0-1]\\.\")]\n",
    "\n",
    "if len(early_layers) > 0 and len(middle_layers) > 0 and len(late_layers) > 0:\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"LAYER DEPTH ANALYSIS:\")\n",
    "    print(\"-\"*80 + \"\\n\")\n",
    "    print(f\"Early layers (0-3): avg long features = {early_layers['long_features_pct'].mean():.1f}%\")\n",
    "    print(f\"Middle layers (4-7): avg long features = {middle_layers['long_features_pct'].mean():.1f}%\")\n",
    "    print(f\"Late layers (8-11): avg long features = {late_layers['long_features_pct'].mean():.1f}%\")\n",
    "    print()\n",
    "\n",
    "# Key insights\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHTS:\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "best_hookpoint = stats_df.iloc[0]\n",
    "worst_hookpoint = stats_df.iloc[-1]\n",
    "\n",
    "print(f\"âœ“ BEST: {best_hookpoint['hookpoint']} has {best_hookpoint['long_features_pct']:.1f}% long features\")\n",
    "print(f\"âœ— WORST: {worst_hookpoint['hookpoint']} has {worst_hookpoint['long_features_pct']:.1f}% long features\")\n",
    "print(f\"ðŸ“Š DIFFERENCE: {best_hookpoint['long_features_pct'] - worst_hookpoint['long_features_pct']:.1f} percentage points\")\n",
    "\n",
    "# Correlation insights\n",
    "high_variance_hp = stats_df.nlargest(1, \"avg_activation_variance\").iloc[0]\n",
    "high_growth_hp = stats_df.nlargest(1, \"avg_growth_ratio\").iloc[0]\n",
    "\n",
    "print(f\"\\nðŸ”¥ HIGHEST activation variance: {high_variance_hp['hookpoint']} ({high_variance_hp['avg_activation_variance']:.4f})\")\n",
    "print(f\"ðŸ“ˆ HIGHEST growth ratio: {high_growth_hp['hookpoint']} ({high_growth_hp['avg_growth_ratio']:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation: Basic Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic validation checks\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VALIDATION CHECKS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Check that all hookpoints were analyzed\n",
    "assert len(all_hookpoint_results) == len(hookpoints_to_test), \\\n",
    "    f\"Expected {len(hookpoints_to_test)} hookpoints, got {len(all_hookpoint_results)}\"\n",
    "print(f\"âœ“ All {len(hookpoints_to_test)} hookpoints were successfully analyzed\")\n",
    "\n",
    "# Check that we got results for all hookpoints\n",
    "hookpoints_with_results = sum(1 for hp, data in all_hookpoint_results.items() \n",
    "                               if len(data.get(\"multi_scale_results\", [])) > 0)\n",
    "print(f\"âœ“ {hookpoints_with_results}/{len(hookpoints_to_test)} hookpoints have multi-scale results\")\n",
    "\n",
    "# Verify DataFrame was created correctly\n",
    "assert len(stats_df) > 0, \"Statistics DataFrame is empty\"\n",
    "print(f\"âœ“ Statistics DataFrame created with {len(stats_df)} rows\")\n",
    "\n",
    "# Verify all expected columns exist\n",
    "expected_columns = [\n",
    "    \"hookpoint\", \"n_latents\", \"avg_dominant_scale\", \"long_features_pct\", \n",
    "    \"avg_scale_score\", \"token_count\", \"phrase_count\", \"sentence_count\", \n",
    "    \"paragraph_count\", \"unknown_count\"\n",
    "]\n",
    "for col in expected_columns:\n",
    "    assert col in stats_df.columns, f\"Missing column: {col}\"\n",
    "print(f\"âœ“ All expected columns present in statistics\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… ALL VALIDATION CHECKS PASSED!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate total time\n",
    "total_pipeline_time = sum(data[\"pipeline_time\"] for data in all_hookpoint_results.values())\n",
    "total_analysis_time = sum(data[\"multi_scale_time\"] for data in all_hookpoint_results.values())\n",
    "print(f\"\\nTotal pipeline time: {total_pipeline_time:.2f} seconds\")\n",
    "print(f\"Total analysis time: {total_analysis_time:.2f} seconds\")\n",
    "print(f\"Grand total: {total_pipeline_time + total_analysis_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
